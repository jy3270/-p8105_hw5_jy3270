---
title: "p8105_hw5_jy3270"
author: "Jingya Yu"
date: "2022-11-16"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


## Problem 1(provided by professor)

The code chunk below imports the data in individual spreadsheets contained in `./data/zip_data/`. To do this, I create a dataframe that includes the list of all files in that directory and the complete path to each file. As a next step, I `map` over paths and import data using the `read_csv` function. Finally, I `unnest` the result of `map`.

```{r}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

The result of the previous code chunk isn't tidy -- data are wide rather than long, and some important variables are included as parts of others. The code chunk below tides the data using string manipulations on the file, converting from wide to long, and selecting relevant variables. 

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

Finally, the code chunk below creates a plot showing individual data, faceted by group. 

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way. 

## Problem 2
```{r}
homicide <- read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")
head(homicide)
```
The dataset has `r nrow(homicide)` observations and `r ncol(homicide)` variables. Each observation is a homicide case, along with the case number, reported date, victim's first and last names, race, age, and sex, the location of the case (city, state, latitude, and longitude), and the disposition of the case.  

- create a city-state variable
```{r}
homicide = homicide %>%
  mutate(city_state = str_c(city, state, sep = ", "))
head(homicide)
```

Summarize within cities to get the total count of homicides and the total count of unsolved homicides.
```{r}
summary = homicide %>%
  group_by(city_state) %>%
  summarize(total_count = n(),
            unsolved_cases = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))) %>% 
 filter(city_state != "Tulsa, AL") 
summary
```

Since Tulsa, Al is a wrong combination of city_state, we choose to eliminate it.

For the city of Baltimore, MD. We use the prop.test function to estimate the proportion of homicides that are unsolved
```{r}
baltimore = summary %>%
  filter(city_state == "Baltimore, MD") 
  
baltimore_proptest = prop.test(baltimore$unsolved_cases, baltimore$total_count) %>% 
  broom::tidy() %>%
  select(estimate, conf.low, conf.high) 
baltimore_proptest
```
For all the cities
```{r}
city_proptests = 
  purrr::map2_df(.x = summary$unsolved_cases,
            .y = summary$total_count,
            ~broom::tidy(prop.test(.x, .y))) %>%
  select(estimate, conf.low, conf.high) %>%
  mutate(city_state = summary$city_state) %>%
  relocate(city_state)
city_proptests
```

Create a plot that shows the estimates and CIs for each city 
```{r}
ggplot(city_proptests, 
       aes(x = estimate, y = fct_reorder(city_state, estimate, .desc=F))) +
  geom_col() +
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high)) + 
  labs(title = "Estimated proportion and CI of unsolved homicides by city",
       x = "proportion of unsolved murders", y = "city_state")
```

## Problem 3
- set the variable
```{r}
#devtools::install_github("simsem/simsem/simsem")
n = 30
sigma = 5
```

generate 5000 datasets from the normal distribution. For each dataset, save the estimate $\hat \mu$ and the p-value arising from a test of H: $\mu = 0$ using $\alpha = 0.05$. 
```{r}
mu = 0
sim_results_mu_0 = 
  expand_grid(mu = mu, 
              iter = 1:5000) %>%
  mutate(sample = map(.x = mu, ~rnorm(n, .x, sigma))) %>%
  mutate(estimate_df = map(.x = sample, 
                           ~broom::tidy(t.test(.x, 
                                               alternative = "two.sided",
                                               conf.level = 0.95)))) %>%
  unnest(estimate_df) %>%
  select(mu, iter, estimate, p.value)
sim_results_mu_0
```

Repeat the above for mu={1,2,3,4,5,6}
```{r}
sim_results_mu_1to6 = 
  expand_grid(mu = 1:6, 
              iter = 1:5000) %>%
  mutate(sample = map(.x = mu, ~rnorm(n, .x, sigma))) %>%
  mutate(estimate_df = map(.x = sample, 
                           ~broom::tidy(t.test(.x, 
                                               alternative = "two.sided",
                                               conf.level = 0.95)))) %>%
  unnest(estimate_df) %>%
  select(mu, iter, estimate, p.value)
sim_results_mu_1to6
```

- A plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of mu on the x axis. 

```{r}
null_rejected = sim_results_mu_1to6 %>%
  group_by(mu) %>%
  summarize(power = sum(p.value < .05)/n()) %>%
  ggplot(aes(x = mu, y = power)) + geom_point() + geom_line() +
  labs(x = "True value of mu", y = "power", title = "true mean vs power")
null_rejected
```

The power increases as mu increases. When the true mean is close to 0, the power is low: when effect size(mu) is 1, the power is about 0.125. When mu is above 4, the power is getting closer and closer to 1.

- A plot showing the average estimate of $\hat \mu$ on the y axis and the true value of mu on the x axis. 
```{r}
mu_average = sim_results_mu_1to6 %>%
  group_by(mu) %>%
  summarize(mean = mean(estimate)) %>%
  ggplot(aes(x = mu, y = mean)) + geom_point() + geom_line() +
  labs(x = "True value of mu", y = "average of estinates", title = "true mean vs average of estinates")
mu_average
```

From the plot above, we can see that the average of all estimate u hat
is roughly equals to the true mean. This also follows our expectation about normal distribution.

Make a second plot (or overlay on the first) the average estimate of $\hat \mu$ only in samples for which the null was rejected on the y axis and the true value of mu on the x axis.

```{r}
null_average = sim_results_mu_1to6 %>%
  group_by(mu) %>%
  filter(p.value < .05) %>%
  summarise(mean = mean(estimate)) %>%
  ggplot(aes(x = mu, y = mean)) + geom_point() + geom_line() +
  labs(x = "True value of mu", y = "average of estinates in samples for which the null was rejected", title = "true mean vs average of estinates in samples for which the null was rejected")
null_average
```

From the plot above, we can see that the sample average of $\hat \mu$ for which the null was rejected does not roughly equal to the true mean when 
mu is small. However, when the effect size is large($\mu$ >= 4), the sample average of $\hat \mu$ for which the null was rejected is roughly equal to the true mean.

This difference might be due to the relationship between effect size and power we find before: when the effect size is small, the power is low, which means that the test only has a small chance of detecting a true effect or that the results are likely to be distorted by random and systematic error. In this case the t test is more likely to fail to reject the null(type 2 error). However, when the effect size is large, the power is high, so the t test are more capable of correctly rejecting the null. As a result, the number of $\hat \mu$ from samples where the null was not rejected when it should is small As we compute the average estimate of $\hat \mu$ only in samples for which the null was rejected and remove those whose null was reject removing them does not move the average of $\hat \mu$ away from the the true value much.